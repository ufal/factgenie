crowdsourcing: |
  In this task, you will annotate textual outputs. For each example, you will see **inputs** on the left side and the corresponding **text** on the right side. Your task is to **highlight spans** in the text according to the instructions.

  These are the span categories you can mark in the generated text:

  {error_list}

  You can annotate the errors by selecting the appropriate error category and dragging your mouse over the text, highlighting the error span.
  
  Once you think you have marked all the errors present in the text, click the **âœ… Mark example as complete** button (you can still update the annotation later). 
  
  You will be able to submit the annotations once they are all are marked as complete.
llm_eval: |
  Given the data:
  ```
  {data}
  ```
  Annotate spans in the following text:
  ```
  {text}
  ```
  Instructions for annotating the text:

  Output the errors as a JSON list "annotations" in which each object contains fields "reason", "text", and "annotation_type". The value of "reason" is the reason for the annotation. The value of "text" is the literal value of the text inside the highlighted span, so that the span can later be identified using string matching. The value of "annotation_type" is an integer index of the error based on the following list:

  {error_list}

  The list should be sorted by the position of the error in the text. Make sure that the annotations are not overlapping.